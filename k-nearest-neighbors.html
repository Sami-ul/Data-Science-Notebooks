<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8" />
    <title>K-Nearest Neighbors</title>
    <style>
      /* General styles */
      body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
      }

      /* Header styles */
      header {
        background-color: #333;
        color: white;
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 1rem;
      }

      header h1 {
        margin: 0;
      }

      /* Container styles */
      .container {
        display: flex;
        align-items: stretch;
        min-height: 100vh;
      }

      /* Model info styles */
      .model-info {
        flex: 1;
        padding: 1rem;
      }

      .model-info h2 {
        font-size: 2rem;
        margin-top: 0;
      }

      .model-info p {
        padding-right: 1rem;
        text-align: justify;
      }

      .model-info a {
        display: inline-block;
        margin-top: 1rem;
        padding: 0.5rem;
        text-decoration: none;
        color: white;
        background-color: #333;
        border-radius: 5px;
        transition: background-color 0.3s ease;
      }

      .model-info a:hover {
        background-color: #555;
      }

      /* Navigation styles */
      .navigation {
        background-color: #f1f1f1;
        width: 25%;
        padding: 1rem;
        position: sticky;
        top: 0;
        display: flex;
        flex-direction: column;
        justify-content: flex-start;
        height: 100vh;
      }

      .navigation h3 {
        font-size: 1.5rem;
        margin-top: 0;
      }

      .navigation ul {
        margin-top: 1rem;
        list-style: none;
        padding: 0;
      }

      .navigation li {
        margin: 0.5rem 0;
      }

      .navigation a {
        display: block;
        padding: 0.5rem;
        text-decoration: none;
        color: #333;
        border-radius: 5px;
        transition: background-color 0.3s ease;
      }

      .navigation a:hover {
        background-color: #ccc;
      }
      iframe[seamless] {
        border: none;
      }
    </style>
  </head>
  <body>
    <header>
      <h1>K-Nearest Neighbors</h1>
    </header>
    <div class="container">
      <div class="navigation">
        <h3>Navigation</h3>
        <ul>
          <li><a href="index.html">Home</a></li>
        </ul>
      </div>
      <div class="model-info">
        <h2>K-Nearest Neighbors (KNN)</h2>
        <h3>Supervised Learning Algorithm | Classification, Regression</h3>
        <ul>
          <li>
            <h4>Introduction</h4>
            <ul>
              <li>
                K-Nearest Neighbors (KNN) is a popular machine learning
                algorithm that is used for classification and regression
                problems.
              </li>
              <li>
                It is a non-parametric method that does not make any assumptions
                about the underlying data distribution, making it useful for a
                wide variety of applications.
              </li>
            </ul>
          </li>
          <li>
            <h4>How it Works</h4>
            <ul>
              <li>
                KNN works by finding the K closest data points to a given input
                data point, based on a distance metric such as Euclidean
                distance.
              </li>
              <li>
                The predicted output for the input data point is then determined
                by the majority class or average value of the K nearest
                neighbors.
              </li>
              <li>
                In the case of regression, the predicted output is the average
                of the K nearest neighbors' output values.
              </li>
            </ul>
          </li>
          <li>
            <h4>Applications</h4>
            <ul>
              <li>
                KNN can be applied to a variety of fields, including computer
                vision, natural language processing, and bioinformatics.
              </li>
              <li>
                It is commonly used for image classification, where the features
                of an image are used to determine its classification.
              </li>
              <li>
                KNN is also used for text classification, where the frequency of
                words in a document is used to determine its classification.
              </li>
              <li>
                In bioinformatics, KNN can be used to classify genes based on
                their expression patterns.
              </li>
            </ul>
          </li>
          <li>
            <h4>Strengths</h4>
            <ul>
              <li>
                KNN has several strengths that make it a useful algorithm:
              </li>
              <li>
                It is simple to implement, making it a good choice for
                prototyping and quick experiments.
              </li>
              <li>
                It works well with both small and large datasets and can be used
                for both classification and regression problems.
              </li>
              <li>
                It is a lazy learning algorithm, meaning that it does not
                require any training and can be updated with new data points at
                any time.
              </li>
            </ul>
          </li>
          <li>
            <h4>Weaknesses</h4>
            <ul>
              <li>KNN also has some weaknesses:</li>
              <li>
                It can be computationally expensive, especially for large
                datasets.
              </li>
              <li>
                It is sensitive to the choice of distance metric, which can have
                a significant impact on the performance of the algorithm.
              </li>
              <li>
                The algorithm is not well-suited for high-dimensional data,
                where the "curse of dimensionality" can lead to inaccurate
                results.
              </li>
            </ul>
          </li>
          <li>
            <h4>Restrictions</h4>
            <ul>
              <li>
                KNN also has some restrictions that should be taken into account
                when using the algorithm:
              </li>
              <li>
                It assumes that all features are equally important, which may
                not always be the case.
              </li>
              <li>
                It assumes that the data points are independent, which may not
                be true in some cases.
              </li>
              <li>
                KNN can be sensitive to the scale of the data, so it is often
                necessary to normalize the features before applying the
                algorithm.
              </li>
            </ul>
          </li>
          <li>
            <h4>Conclusion</h4>
            <ul>
              <li>
                KNN is a simple and versatile machine learning algorithm that
                can be used for both classification and regression problems.
              </li>
              <li>
                It is a non-parametric algorithm that does not make any
                assumptions about the underlying data distribution, making it
                useful for a wide variety of applications.
              </li>
              <li>
                It is a lazy learning algorithm, meaning that it does not
                require any training and can be updated with new data points at
                any time.
              </li>
              <li>
                However, it can be computationally expensive for large datasets
                and is sensitive to the choice of distance metric.
              </li>
            </ul>
          </li>
        </ul>
        <h3>Application</h3>
        <iframe
          src="notebooks\K-Nearest Neighbors Application - Sami-ul Ahmed.html"
          seamless
          style="width: 100%; height: 100%"
          scrolling="auto"
        ></iframe>
      </div>
    </div>
  </body>
</html>
